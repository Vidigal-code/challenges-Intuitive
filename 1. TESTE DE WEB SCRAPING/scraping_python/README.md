# üï∏Ô∏è scraping_python - Multidocumentos

## üìÑ Descri√ß√£o

Este projeto √© um **web scraper em Python** que automatiza o processo de **download de arquivos PDF** a partir de p√°ginas p√∫blicas, como a da [ANS](https://www.gov.br/ans/), e outros √≥rg√£os governamentais. Ele permite buscar v√°rios anexos com base em **express√µes regulares configur√°veis**, salv√°-los em diret√≥rios espec√≠ficos e compact√°-los em arquivos `.zip` ‚Äî tudo isso com um simples comando!

## üìÅ Estrutura do Projeto

```
scraping_python/
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ config.json                # Arquivo de configura√ß√£o com m√∫ltiplas fontes
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ compressor/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ file_compressor.py     # Classe para compactar arquivos em ZIP
‚îÇ   ‚îú‚îÄ‚îÄ directory/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ directory_manager.py   # Classe para criar diret√≥rios
‚îÇ   ‚îî‚îÄ‚îÄ downloader/
‚îÇ       ‚îî‚îÄ‚îÄ pdf_downloader.py      # Classe para baixar os arquivos PDF
‚îÇ
‚îú‚îÄ‚îÄ main.py                        # Script principal
‚îú‚îÄ‚îÄ requirements.txt               # Depend√™ncias do projeto
‚îî‚îÄ‚îÄ README.md                      # Documenta√ß√£o do projeto
```

---

## ‚öôÔ∏è Requisitos

- Python **3.6+**
- Bibliotecas Python:
  - `requests`
  - `beautifulsoup4`

### Instala√ß√£o

```bash
git clone https://github.com/seu-usuario/scraping_python.git
cd scraping_python
pip install -r requirements.txt
```

Ou manualmente:

```bash
pip install requests beautifulsoup4
```

---

## üîß Configura√ß√£o

O scraper utiliza um arquivo de configura√ß√£o JSON localizado em `config/config.json`. Este arquivo define **v√°rias fontes de download**, seus diret√≥rios e os anexos que devem ser buscados.

### Exemplo de configura√ß√£o com m√∫ltiplas fontes:

```json
[
  {
    "url": "https://www.gov.br/ans/pt-br/acesso-a-informacao/participacao-da-sociedade/atualizacao-do-rol-de-procedimentos",
    "download_directory": "anexos_ans",
    "output_zip": "anexos_ans.zip",
    "anexos": [
      {
        "nome": "Anexo I",
        "regex": "anexo\\s*i"
      },
      {
        "nome": "Anexo II",
        "regex": "anexo\\s*ii"
      }
    ]
  },
  {
    "url": "https://www.gov.br/outro-orgao/documentos-importantes",
    "download_directory": "documentos_ministerio",
    "output_zip": "documentos_2025.zip",
    "anexos": [
      {
        "nome": "Relat√≥rio Anual",
        "regex": "relat[o√≥]rio\\s*anual"
      },
      {
        "nome": "Or√ßamento",
        "regex": "or[c√ß]amento\\s*2025"
      },
      {
        "nome": "Plano Estrat√©gico",
        "regex": "plano\\s*estrat[e√©]gico"
      }
    ]
  }
]
```

> ‚ö° **Dica**: Modifique o arquivo JSON para apontar para qualquer outro site com PDFs p√∫blicos e adapte as express√µes regulares conforme necess√°rio.

---

## üöÄ Uso

Basta executar o script principal para iniciar o processo de scraping:

```bash
python main.py
```

O programa ir√°:

1. Criar os diret√≥rios de destino
2. Acessar cada URL configurada
3. Localizar os links para os anexos baseando-se nas regex configuradas
4. Baixar os PDFs encontrados
5. Compactar os arquivos em um ZIP no diret√≥rio correspondente

---

## üîÑ Flexibilidade

Este projeto √© altamente reutiliz√°vel, com foco no conceito **"configura√ß√£o sobre c√≥digo"**. Ou seja, voc√™ pode reutilizar o mesmo c√≥digo para:

- Baixar relat√≥rios de diversos √≥rg√£os
- Manter arquivos organizados por tema
- Realizar auditorias automatizadas de documentos p√∫blicos

---

## ‚úèÔ∏è Personaliza√ß√£o

Para adicionar novos anexos ou fontes:

1. Edite `config/config.json`
2. Insira uma nova entrada com:
   - `"url"`: a p√°gina que cont√©m os anexos
   - `"download_directory"`: onde os arquivos ser√£o salvos
   - `"output_zip"`: nome do arquivo ZIP final
   - `"anexos"`: lista de objetos com `nome` e `regex`

---

## ü§ù Contribui√ß√£o

Pull requests s√£o bem-vindos! Sinta-se √† vontade para:

- Reportar bugs
- Sugerir melhorias
- Adicionar novos recursos

---

## üìú Licen√ßa

Este projeto √© livre para fins de **teste, estudo e aprendizado**. Use com responsabilidade ao acessar conte√∫dos de sites p√∫blicos.


